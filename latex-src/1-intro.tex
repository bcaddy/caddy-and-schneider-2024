\section{Introduction}
\label{sec:intro}

% ==============================================================================
% Outline:
% - why do big sims matter?
%   - esp fast efficient MHD codes
%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new 
%     computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes 
%     it easier and faster to dev since they don't have to worry about breaking 
%     other peoples stuff
% - Testing/CI stuff
% - outline paper
%   - alude to performance and testing at scale
% ==============================================================================

% - why do big sims matter?
%   - esp fast efficient MHD codes
Over the past decade it has become increasingly clear that magnetohydrodynamics (MHD) plays a significant role in a variety of astrophysical phenomenon\citep{teyssier_2015, naab_2017, tumlinson_2017, han_2017}. Magnetic fields couple to gas both directly through plasma interactions with the magnetic field and indirectly through cosmic ray transport\citep{werhahn_gamma-ray_2023, yoshida_trajectory_2021}, anisotropic conduction\citep{bruggen_2023}, and other physical effects.

% cite Arepo, Auriga, TNG, specific ones that I've talked about before

% I'd start this paragraph by bringing up the science problem, ideally linking it back to something you mentioned in the first paragraph of the introduction. Then you can explain why high resolution simulations are required to solve it.

% rephrase this less critically. I'd reframe the whole thing to suggest that these studies have provided intriguing hints of the possible effects of magnetic fields, but the results are not converged.
The role of magnetic fields in galaxy dynamics is a matter of significant debate in the scientific literature. Different simulation methods, codes, and resolutions all give different results which range from magnetic fields being largely irrelevant up to magnetic fields being critically important for the evolution of the interstellar medium (ISM), galactic feedback, and the structure of the circumgalactic medium (CGM)\citep{pakmor_faraday_2018,pakmor_simulations_2013,pakmor_magnetic_2017,pakmor_magnetizing_2020,ntormousi_dynamo_2020,van_de_voort_effect_2021}. Many simulation projects, notably the Auriga Project\citep{grand_auriga_2017}, along with various followup analysis and simulations \citep{pakmor_faraday_2018,pakmor_simulations_2013,pakmor_magnetic_2017,pakmor_magnetizing_2020,ntormousi_dynamo_2020,van_de_voort_effect_2021} have attempted to determine the role of magnetic fields in galactic dynamics and have provided intriguing hints as to the possible effects of magnetic fields in galaxy dynamics. However, the lingering disagreements about the exact impacts of magnetic fields show that our understanding is not complete. A major reason for this is that galactic magnetic fields are likely amplified by a turbulent dynamo which operates over a large dynamic range\citep{beck_1996, carteret_2022, brandenburg_2022}. To resolve this dynamo requires extremely high resolution MHD simulation that are now possible using the MHD module of Cholla.

Modern numerical methods for MHD are sophisticated and robust, but even with highly optimized codes, MHD simulations remain very computationally expensive\cite{athena++_2020}. This computational expense is due the high number of floating point calculations required by modern finite-volume methods and the heavy memory bandwidth demands of MHD codes\citep{k_athena_2021}. In addition, MHD turbulent dynamos operate across a large dynamic range, so high resolution simulations are critical to accurately capture the effects of magnetohydrodynamics in astrophysical simulations \citep{galishnikova_tearing_2022, pakmor_simulations_2013}. The computational expense of MHD simulations, along with the advent of Graphics Processing Units (GPUs) as the primary source of computational power in new cutting edge supercomputers\footnote{https://www.top500.org}, thus necessitates the development of GPU-based astrophysical MHD simulation codes. 

The induction equation requires that the magnetic field be divergence free, i.e. the Universe does not contain magnetic monopoles. However, not all numerical methods for evolving the MHD equations maintain this constraint. In many particle-based schemes, for example, magnetic divergence is generated and is removed at each step, a method commonly known as divergence cleaning\citep{dedner_hyperbolic_2002}. and constrained transport \citep{evans_1988}. Divergence cleaning essentially functions by computing the divergence regularly and subtracting it away. Divergence cleaning is computationally cheaper but typically leads to divergence errors on the level of a few percent \citep{pakmor_magnetizing_2020,van_de_voort_effect_2021}. While this error is small it could lead to unphysical solutions and so an alternative was sought. The other primary method for evolving the magnetic field in grid based codes is constrained transport (CT). Constrained transport is formally divergence free, and when implemented numerically it typically results in divergence errors on the order of machine round off error\citep{evans_1988,stone_athena_2008, stone_2009}. This is accomplished by tracking magnetic fields on a staggered, face centered grid rather than using cell-centered averages. These face centered values are used in conjunction with Riemann fluxes to calculate edge centered electric fields, and those electric fields are used to update the magnetic field\citep{evans_1988,stone_athena_2008, stone_2009}. Thus, the trade-off for a divergence-free method is significant additional algorithmic complexity and associate computational expense.

% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes it easier and faster to dev since they don't have to worry about breaking other peoples stuff
% Note that K-Athena is not related to Athena and is no longer under development, it does use CT. AthenaPK is under active development and uses divergence cleaning
The Cholla code (Computational Hydrodynamics On paraLLel Architectures)\citep{schneider_2015} is a fixed grid, finite volume hydrodynamics code for astrophysics that was designed to run natively on GPU-based supercomputers. It employs an unsplit 3D hydrodynamics integrator based on the Van Leer predictor-corrector method \citep{falle_1991, van_leer_2003} and was designed to be extended to MHD using constrained transport\citep{evans_1988, stone_athena_2008}. This work presents the MHD extension of Cholla. Our MHD implementation largely follows the the Van Leer + Constrained Transport (VL+CT) method presented in \cite{stone_2009} with modifications for GPUs. It also utilizes an HLLD Riemann solver\citep{hlld_2005} and includes second\citep{stone_2009} and third\citep{felker_2018} order reconstruction in the characteristic variables\citep{stone_athena_2008}.

%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
The extension of Cholla to include MHD allows the simulation of previously unreachable domains. The VL+CT integrator provides high accuracy results with divergences that are zero to round off error. Given current memory constraints, the code is fast enough that a $\sim450^3$ cell MHD simulation can be run on a single high end GPU, allowing research-quality simulations to be run with only a small number of local resources. Alternatively, Cholla scales up to power of the largest available supercomputers, enabling simulations up to $19,278^3 \approx 7.2 \text{trillion}$ cells on Frontier\footnote{https://www.olcf.ornl.gov/frontier/}. This will allow MHD simulations of entire galaxies with a constant resolution of a few parsecs per cell, turbulent box simulations with many trillions of cells, or parameter studies of thousands of lower resolution simulations to be computed rapidly.

% - Testing/CI stuff
"In addition to requiring complex algorithms to produce accurate results, modern community-developed simulation codes like Cholla require robust testing and software-development infrastructure to maintain their reliability. We also describe the implementation of an automated testing/continuous integration (CI) pipeline for Cholla. CI tools have expanded rapidly in functionality and popularity over the last 20 years and their usefulness in scientific software is well established\citep{beck_1999, wilson_2014,wilson_2017}. Particularly in the last 5 years with the advent of GitHub Actions and similar easily accessible and cheap (or even free) tools, CI pipelines have become much more straightforward to set up and run even for small groups and individuals. Despite their benefits and overall popularity in industry, the the scientific community has been slow to adopt these tools.

As the complexity of Cholla has grown and more developers and collaborators have started actively contributing to the code base, the necessity of adopting many scientific software best practices has become increasingly clear. In Section \ref{sec:testing} we present our implementation of testing and CI that is easy to use and designed from the ground up to be scalable from a single GPU all the way up to an exascale machine. Our approach works using a hybrid of several different tools and has allowed much more rapid and confident development on Cholla with less risk of introducing bugs in existing code. Overall, the addition of robust testing to Cholla is a significant advancement in guaranteeing correctness and the long term extensibility of the code.

% - outline paper
%   - alude to performance and testing at scale
The outline of this paper is as follows. In Section \ref{sec:methods}, we describe the VL+CT algorithm in detail along with the modifications we made to efficiently run on GPUs. In Section \ref{sec:mhd-tests}, we demonstrate the correctness and accuracy of the of Cholla on a suite of MHD test problems. We also  describe the Cholla's performance and weak scaling behavior on up to 74,088 GPUs using Frontier, the world's first exascale supercomputer. In Section \ref{sec:testing}, we discuss the new continuous integration and automated testing framework. We conclude in Section \ref{sec:summary}. 
