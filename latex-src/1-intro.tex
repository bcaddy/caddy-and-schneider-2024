\section{Introduction}
\label{sec:intro}

% ==============================================================================
% Outline:
% - why do big sims matter?
%   - esp fast efficient MHD codes
%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new 
%     computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes 
%     it easier and faster to dev since they don't have to worry about breaking 
%     other peoples stuff
% - Testing/CI stuff
% - outline paper
%   - alude to performance and testing at scale
% ==============================================================================

% - why do big sims matter?
%   - esp fast efficient MHD codes
Over the past decade it has become increasingly clear that magnetohydrodynamics (MHD) plays a significant role in a variety of astrophysical phenomenon\citep{kobayashi_metallicity_2023, wang_three-dimensional_2023, mori_scale-dependent_2023, werhahn_gamma-ray_2023, zhang_particle_2023, varma_3d_2023, stimpson_numerical_2023, bruggen_2023, yoshida_trajectory_2021, kim_introducing_2023, jones_density_2023, lu_effect_2020,kortgen_shape_2019,galishnikova_tearing_2022, su_stellar_2018,roy_seeding_2023,fielding_how_2017}. Magnetic fields couple to gas both directly through plasma interactions with the magnetic field and indirectly through cosmic ray transport\citep{werhahn_gamma-ray_2023, yoshida_trajectory_2021}, anisotropic conduction\citep{bruggen_2023}, and other physical effects.

Modern numerical methods for MHD are sophisticated and robust, but even with highly optimized codes, MHD simulations remain very computationally expensive\cite{athena++_2020}. This computational expense is due to the complexity of MHD interactions, the divergence free condition, and the heavy memory bandwidth demands of MHD codes\citep{k_athena_2021}. In addition, MHD turbulent dynamos operate across all length scales, so high resolution simulations are critical to accurately capture the effects of magnetohydrodynamics in astrophysical simulations \citep{galishnikova_tearing_2022, pakmor_simulations_2013}. The computational expense of MHD simulations, along with the advent of Graphics Processing Units (GPUs) as the primary source of computational power in new cutting edge supercomputers\footnote{https://www.top500.org}, thus necessitates the development of GPU-based astrophysical MHD simulation codes. 

% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes it easier and faster to dev since they don't have to worry about breaking other peoples stuff
% Note that K-Athena is not related to Athena and is no longer under development, it does use CT. AthenaPK is under active development and uses divergence cleaning
The Cholla code (Computational Hydrodynamics On paraLLel Architectures)\citep{schneider_2015} is a fixed grid, finite volume hydrodynamics code for astrophysics that was designed to run natively on GPU-based supercomputers. It employs an unsplit 3D hydrodynamics integrator based on the Van Leer predictor-corrector method  and was designed to be extended to MHD using constrained transport. This work presents the MHD extension of Cholla. Our MHD implementation largely follows the the Van Leer + Constrained Transport (VL+CT) method presented in \cite{stone_2009} with modifications for GPUs. It also utilizes an HLLD Riemann solver and includes second and third order reconstruction in the characteristic variables.

Without any special care standard MHD methods often generate non-zero divergences, i.e. magnetic monopoles. The two main methods of dealing with this are divergence cleaning \citep{dedner_hyperbolic_2002} and constrained transport \citep{evans_1988}. Divergence cleaning essentially functions by computing the divergence regularly and subtracting it away. Divergence cleaning is computationally cheaper but typically leads to divergence errors on the level of a few percent \citep{pakmor_magnetizing_2020,van_de_voort_effect_2021}. While this error is small it is not inconsequential and so the more accurate and more computationally expensive constrained transport is often preferred. Constrained transport, on the other hand is formally divergence free, and when implemented numerically it typically results in divergence errors on the order of machine round off error\citep{evans_1988,stone_athena_2008, stone_2009}. Thiss is accomplished by tracking magnetic fields on a staggered, face centered grid rather than using cell-centered averages. These face centered values are used in conjunction with Riemann fluxes to calculate edge centered electric fields, and those electric fields are used to update the magnetic field\citep{evans_1988,stone_athena_2008, stone_2009}. Thus, the tradeoff for a divergence-free method is significant additional algorithmic complexity and associate computational expense.

%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
The extension of Cholla to include MHD allows the simulation of previously unreachable domains. The VL+CT integrator provides high accuracy results with divergences that are zero to round off error. It is performant enough that a single high end GPU can run a $459^3$ cell MHD simulation rapidly or many GPUs can work together on a cluster for simulations of up to $19,278^3 \approx 7.2 \text{trillion}$ cells on Frontier\footnote{https://www.olcf.ornl.gov/frontier/}. This will allow MHD simulations of entire galaxies with a constant resolution of a few parsecs per cell, turbulent box simulations with many trillions of cells, or parameter studies of thousands of lower resolution simulations to be computed rapidly.

% cite Arepo, Auriga, TNG, specific ones that I've talked about before
High resolution global MHD simulations of galaxies that can achieve resolutions of a few parsecs are especially important because there is little scientific consensus on the role of magnetic fields in galaxy dynamics. Different simulation methods, codes, and resolutions all give slightly different results which range from magnetic fields being largely irrelevant up to magnetic fields being critically important for the evolution of the interstellar medium (ISM), galactic feedback, and the structure of the circumgalactic medium (CGM)\citep{pakmor_faraday_2018,pakmor_simulations_2013,pakmor_magnetic_2017,pakmor_magnetizing_2020,ntormousi_dynamo_2020,van_de_voort_effect_2021}. Various simulation projects, notably the Auriga Project\citep{grand_auriga_2017} along with various followup analysis and simulations \citep{pakmor_faraday_2018,pakmor_simulations_2013,pakmor_magnetic_2017,pakmor_magnetizing_2020,ntormousi_dynamo_2020,van_de_voort_effect_2021} have attempted to answer this question but have all run into issues with insufficient resolution to simulate the turbulent dynamo in detail, noticeable divergence errors due to using divergence cleaning, or both. In contrast, Cholla MHD can perform global MHD galaxy simulations with resolution on the order of a single parsec while using the divergence free constrained transport method.

% - Testing/CI stuff
We also describe the implentation of an automated testing/continuous integration (CI) pipeline for Cholla. CI tools have expanded rapidly in functionality and popularity over the last 20 years and their usefulness in scientific software is well established\citep{beck_1999, wilson_2014,wilson_2017}. Particularly in the last 5 years with the advent of GitHub Actions and similar easily accesible tools, CI pipelines have become much more straightforward to set up and run even for small groups and individuals. Despite their benefits and overall popularity, the the scientific community has been slow to adopt these tools and when these, and other best practices, are adopted they are often slow, difficult to use, or not very thorough. 

As the complexity of Cholla has grown and more developers and collaborators have started actively contributing to the code base, the necessity of adopting many scientific software best practices has become increasingly clear. In Section \ref{sec:testing} we present our implementation of testing and CI that is easy to use and designed from the ground up to be scalable from a single GPU all the way up to an exascale machine. Our approach works using a hybrid of several different tools and has allowed much more rapid and confident development on Cholla with less risk of introducing bugs in existing code. Overall, the addition of robust testing to Cholla is a significant advancement in guaranteeing correctness and the long term extensibility of the code.

% - outline paper
%   - alude to performance and testing at scale
The outline of this paper is as follows. In Section \ref{sec:methods}, we describe the VL+CT algorithm in detail along with the modifications we made to efficiently run on GPUs. In Section \ref{sec:mhd-tests}, we demonstrate the correctness and accuracy of the of Cholla on a suite of MHD test problems. We also  describe the Cholla's performance and weak scaling behavior on up to 74,088 GPUs using Frontier, the world's first exascale supercomputer. In Section \ref{sec:testing}, we discuss the new continous integration and automated testing framework. We conclude in Section \ref{sec:summary}. 

In figure captions the GitHub icon, \img{../assets/github.png}, serves as a link to the version of the python script which generated that figure. These scripts, and the associated GitHub repository, have sufficient information to reproduce the figures shown in this paper.