\section{Introduction}
\label{sec:intro}

% ==============================================================================
% Outline:
% - why do big sims matter?
%   - esp fast efficient MHD codes
%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new 
%     computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes 
%     it easier and faster to dev since they don't have to worry about breaking 
%     other peoples stuff
% - Testing/CI stuff
% - outline paper
%   - alude to performance and testing at scale
% ==============================================================================

% - why do big sims matter?
%   - esp fast efficient MHD codes
Over the past decade it has become increasingly clear that magnetohydrodynamics (MHD) plays a significant role in a variety of astrophysical phenomenon\citep{kobayashi_metallicity_2023, wang_three-dimensional_2023, mori_scale-dependent_2023, werhahn_gamma-ray_2023, zhang_particle_2023, varma_3d_2023, stimpson_numerical_2023, bruggen_2023, yoshida_trajectory_2021, kim_introducing_2023, jones_density_2023, lu_effect_2020,kortgen_shape_2019,galishnikova_tearing_2022, su_stellar_2018,roy_seeding_2023,fielding_how_2017}. Magnetic fields are coupled to the gas both directly through plasma interactions with the magnetic field and indirectly through cosmic ray transport\citep{werhahn_gamma-ray_2023, yoshida_trajectory_2021}, anisotropic conduction\citep{bruggen_2023}, and other similar effects. 

Modern MHD methods are sophisticated and robust but nonetheless MHD simulations remain very computationally expensive\cite{athena++_2020}. This computational expense is due to the complexity of MHD interactions, the divergence free condition, and the heavy memory bandwidth demands of MHD codes\citep{k_athena_2021}. In addition, MHD turbulent dynamos operate across all length scales and so high resolution simulations are critically important for accurately simulating astrophysical magnetohydrodynamics\citep{galishnikova_tearing_2022, pakmor_simulations_2013}. The computational expense of MHD simulations, along with the advent of Graphics Processing Units (GPUs) as the primary source of computational power in new cutting edge supercomputers\footnote{https://www.top500.org}, necessitates the development of GPU based astrophysical MHD simulation codes. 

% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes it easier and faster to dev since they don't have to worry about breaking other peoples stuff
% Note that K-Athena is not related to Athena and is no longer under development, it does use CT. AthenaPK is under active development and uses divergence cleaning
The Cholla code (Computational Hydrodynamics On paraLLel Architectures)\citep{schneider_2015} is a massively parallel, finite volume, fixed grid, GPU native, astrophysical hydrodynamics code that was designed from the ground up to run on massive GPU supercomputers and be extended to MHD. This work presents the MHD extension of Cholla. The MHD integrator is based off the Van Leer + Constrained Transport (VL+CT) integrator of \cite{stone_2009} with modifications for GPUs. It also utilizes the HLLD Riemann solver and includes second and third order reconstruction in the characteristic variables.

Without any special care standard MHD methods often generate non-zero divergences, i.e. magnetic monopoles. The two main methods of dealing with this are divergence cleaning \citep{dedner_hyperbolic_2002} and constrained transport \citep{evans_1988}. Divergence cleaning essentially functions by computing the divergence regularly and subtracting it away. Divergence cleaning is computationally cheaper but typically leads to divergence errors on the level of a few percent \citep{pakmor_2020}. While this error is small it is not inconsequential and so the more accurate and more computationally expensive constrained transport is often preferred. Constrained transport, while more computationally expensive, typically gives divergence errors on the order of machine round off error\citep{evans_1988,stone_athena_2008, stone_2009}. It does this by utilizing a staggered, face centered grid for the magnetic field, then taking the magnetic fluxes returned by the Riemann solver, converting them to edge centered electric fields, and using those electric fields to update the magnetic field\citep{evans_1988,stone_athena_2008, stone_2009}.

%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
Cholla with MHD allows the simulation of previously unreachable domains. The VL+CT integrator provides high accuracy results with divergences that are zero to round off error. It is performant enough that a single GPU can run a $459^3$ cell MHD simulation rapidly or many GPUs can work together on a cluster for simulations of up to $19,278^3 \approx 7.2 \text{trillion}$ cells on Frontier\footnote{https://www.olcf.ornl.gov/frontier/}. This allows MHD simulations of entire galaxies with a constant resolution of a few parsecs per cell, turbulent box simulations with many trillions of cells, or parameter studies of lower resolution to be computed rapidly.

% cite Arepo, Auriga, TNG, specific ones that I've talked about before
High resolution global MHD simulations of galaxies that can achieve resolutions of a few parsecs are especially important because there is little scientific consensus on the role of magnetic fields in galaxy dynamics. Different simulation methods, codes, and resolutions all give slightly different results which range from magnetic fields being largely irrelevant up to magnetic fields being critically important\citep{pakmor_faraday_2018,pakmor_simulations_2013,pakmor_magnetic_2017,pakmor_magnetizing_2020,ntormousi_dynamo_2020,van_de_voort_effect_2021}. Various simulation projects, notably the Auriga Project\citep{grand_auriga_2017} along with various followup analysis and simulations \citep{pakmor_faraday_2018,pakmor_simulations_2013,pakmor_magnetic_2017,pakmor_magnetizing_2020,ntormousi_dynamo_2020,van_de_voort_effect_2021} have attempted to answer this question but have all run into issues with insufficient resolution to simulate the turbulent dynamo in detail, noticeable divergence errors due to using divergence cleaning, or both. In contrast, Cholla MHD can perform global MHD galaxy simulations with resolution on the order of a single parsec while using the divergence free constrained transport method.

% - Testing/CI stuff
We also present on the usage of an automated testing/continuous integration (CI) pipeline in Cholla. CI tools have expanded rapidly in functionality and popularity over the last 20 years and its usefulness in scientific software is well established\citep{beck_1999, wilson_2014,wilson_2017}. In the last 5 years especially, with the advent of GitHub Actions and similar tools, CI pipelines have become easy to setup and run even for small groups and individuals. Despite the benefits and overall popularity of these tools the scientific community has been slow to adopt them and when these, and other best practices, are adopted they are often slow, difficult to use, or not very thorough. 

As the complexity of Cholla grew and more developers and collaborators started working with Cholla the necessity of adopting many scientific software best practices became paramount. To this end, in Section \ref{sec:testing} we present our implementation of testing and CI that is easy to use and designed from the ground up to be scalable from a single GPU all the way up to an exascale machine. Our approach works using a hybrid of several different tools and has allowed much more rapid and confident development on Cholla with less risk of bugs. Overall, the addition of robust testing to Cholla is a significant advancement in guaranteeing correctness and the long term extensibility of the code.

% - outline paper
%   - alude to performance and testing at scale
In Section \ref{sec:methods} we describe the VL+CT algorithm in detail along with the modifications required to utilize GPUs. 
In Section \ref{sec:mhd-tests} we show a suite of MHD test problems to demonstrate the correctness and accuracy of the code and demonstrate the performance and weak scaling on up to 74,088 GPUs.
In Section \ref{sec:testing} we discuss the new testing framework. 
We conclude in Section \ref{sec:summary}. 

In figure captions the GitHub icon, \img{../assets/github.png}, serves as a link to the version of the python script which generated that figure. These scripts, and the associated GitHub repository, have sufficient information to reproduce the figures shown in this paper.