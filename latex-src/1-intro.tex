\section{Introduction}
\label{sec:intro}

% ==============================================================================
% Outline:
% - why do big sims matter?
%   - esp fast efficient MHD codes
%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new 
%     computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes 
%     it easier and faster to dev since they don't have to worry about breaking 
%     other peoples stuff
% - Testing/CI stuff
% - outline paper
%   - alude to performance and testing at scale
% ==============================================================================

% - why do big sims matter?
%   - esp fast efficient MHD codes
Over the past decade it has become increasingly clear that magnetohydrodynamics (MHD) plays a significant role in a variety of astrophysical phenomena \citep[e.g.][]{shin_2008, beck_2016, banda_2016, naab_2017, han_2017, ntormousi_dynamo_2020, wibking_2021, gent_2021}. Magnetic fields couple to gas both directly through plasma interactions with the magnetic field, and indirectly through cosmic ray transport \citep[e.g.][]{pfrommer_simulating_2017, girichidis_spectrally_2019, chan_cosmic_2019, buck_effects_2020, werhahn_cosmic_2021, yoshida_trajectory_2021, girichidis_spectrally_2022, nunez-castineyra_cosmic-ray_2022, werhahn_gamma-ray_2023}, anisotropic conduction \citep[e.g.][]{yang_fermi_2012, hanasz_cosmic_2013, simpson_role_2016, girichidis_launching_2016, pakmor_galactic_2016, bruggen_2023}, and other physical effects.

% cite Arepo, Auriga, TNG, specific ones that I've talked about before

% I'd start this paragraph by bringing up the science problem, ideally linking it back to something you mentioned in the first paragraph of the introduction. Then you can explain why high resolution simulations are required to solve it.

% rephrase this less critically. I'd reframe the whole thing to suggest that these studies have provided intriguing hints of the possible effects of magnetic fields, but the results are not converged.
The role of magnetic fields in galactic structure is a matter of significant debate in the scientific literature. Different simulation methods, codes, and resolutions show varying impacts of magnetic fields, ranging from magnetic fields being largely irrelevant to large scale structure to magnetic fields being critically important in determining the evolution and structure of the interstellar medium (ISM), modifying galactic feedback, and the influencing the structure of the circumgalactic medium (CGM) \citep{banda_2016, pakmor_magnetic_2017, naab_2017, pakmor_faraday_2018, pakmor_magnetizing_2020, ntormousi_dynamo_2020, wibking_2021, van_de_voort_effect_2021}.

Many simulation projects, notably the Auriga Project \citep{grand_auriga_2017}, along with various followup analysis and simulations \citep{pakmor_faraday_2018,pakmor_simulations_2013,pakmor_magnetic_2017,pakmor_magnetizing_2020,ntormousi_dynamo_2020, hopkins_but_2020, van_de_voort_effect_2021} have attempted to determine the role of magnetic fields in galactic structure and have provided intriguing hints as to the possible effects of magnetic fields in galaxy dynamics. However, the lingering disagreements about the exact impacts of magnetic fields show that our understanding is not complete. A major reason for this uncertainty is the effect of numerical resolution in MHD simulations -- galactic magnetic fields are likely amplified by a turbulent dynamo which operates over a large dynamic range \citep{beck_1996, carteret_2022, brandenburg_2022}. Resolving this dynamo will require extremely high resolution MHD simulations -- simulations that are now possible thanks to recent developments in hardware, numerical algorithms, and software.

Modern numerical methods for MHD are sophisticated and robust, but even with highly optimized codes, MHD simulations remain very computationally expensive \citep{athena++_2020}. This computational expense is a result of the high number of floating point calculations required by modern finite-volume methods and the heavy memory bandwidth demands of MHD codes \citep{k_athena_2021}. In addition, MHD turbulent dynamos operate across a large dynamic range, from the full scale of a galaxy all the way down to a few parsecs or smaller, five or more orders of magnitude spatially \citep{ galishnikova_tearing_2022}. As a result, high resolution simulations are critical in order to accurately capture the effects of magnetic fields in astrophysical simulations. This need has driven a push to develop MHD codes that can take advantage of modern computer architectures \citep[e.g.][]{schive_gamer-2_2018, almgren_castro_2020, zingale_castro_2020, shankar_gram-x_2022, liska_h-amr_2022, begue_cuharm_2023, holmen_early_2023}. The computational expense of MHD simulations, along with the advent of Graphics Processing Units (GPUs) as the primary source of computational power in new cutting edge supercomputers\footnote{https://www.top500.org}, thus necessitates the development of GPU-based astrophysical MHD simulation codes. 

The induction equation requires that the magnetic field be divergence free, i.e. the Universe does not contain magnetic monopoles. However, not all numerical methods for evolving the MHD equations maintain this constraint. In many particle-based schemes, for example, magnetic divergence is generated and is removed at each step, a method commonly known as divergence cleaning \citep{dedner_hyperbolic_2002}. Divergence cleaning is popular because it couples well to particle-based methods -- it essentially functions by computing the divergence regularly and subtracting it away. Divergence cleaning is also computationally cheaper than numerical methods that maintain a divergence-free solution but typically leads to divergence errors on the level of a few percent \citep{dedner_hyperbolic_2002, pakmor_magnetizing_2020, van_de_voort_effect_2021}. While this error is small it could lead to unphysical solutions. However, a more accurate method for evolving the magnetic fields exists for grid based codes.

The other primary method for evolving the magnetic field in grid based codes is constrained transport (CT). Constrained transport is formally divergence free, and when implemented numerically it typically results in divergence errors on the order of machine round off error \citep{evans_1988, gardiner_2005, stone_athena_2008, stone_2009, zingale_castro_2020, almgren_castro_2020}. This is accomplished by tracking magnetic fields on a staggered, face centered grid rather than using cell-centered averages. These face centered values are used in conjunction with Riemann fluxes to calculate edge centered electric fields, and those electric fields are used to update the magnetic field. Thus, the trade-off for a divergence-free method is significant additional algorithmic complexity and associated computational expense.

% - why this code? 
%   - why a new code? Cholla is fast, scales well, GPUs are fast, all big new computers are gpu based
%   - finite volume + CT, what other methods? Why is divergence cleaning bad
%   - cite lots of other codes and why is our code different
%   - why do we use this framework, what other options are there
%   - we have a testing framework that allows for scalable development, makes it easier and faster to dev since they don't have to worry about breaking other peoples stuff
% Note that K-Athena is not related to Athena and is no longer under development, it does use CT. AthenaPK is under active development and uses divergence cleaning
The Cholla code (Computational Hydrodynamics On paraLLel Architectures) \citep{schneider_2015} is a fixed grid, finite volume hydrodynamics code for astrophysics that was designed to run natively on GPU-based supercomputers. It employs an unsplit 3D hydrodynamics integrator based on the Van Leer predictor-corrector method \citep{falle_1991, van_leer_2006} and was designed to be extended to MHD using constrained transport \citep{evans_1988, stone_athena_2008}. This work presents the MHD extension of Cholla. Our MHD implementation largely follows the Van Leer + Constrained Transport (VL+CT) method presented in \cite{stone_2009} with modifications for GPUs. It also uses an HLLD Riemann solver \citep{hlld_2005} and includes second \citep{stone_2009} and third \citep{felker_2018} order reconstruction in the characteristic variables \citep{stone_athena_2008}.

%   - what kinds of sims?
%     - giant turbulent boxes, mhd outflows of galaxies
The extension of Cholla to include MHD allows the simulation of previously unreachable domains. The VL+CT integrator provides high accuracy results with divergences that are zero to round off error. Given current memory constraints, the code is fast enough that a $\sim450^3$ cell MHD simulation can be run on a single high end GPU, allowing research-quality simulations to be run with only a small number of local resources. In addition, Cholla scales up to power of the largest available supercomputers, enabling simulations up to $19,278^3 \approx 7.2$ trillion cells on \textit{Frontier}\footnote{https://www.olcf.ornl.gov/frontier/}, the world's first exascale supercomputer. This will allow MHD simulations of entire galaxies with a constant resolution of a few parsecs per cell, turbulent box simulations with many trillions of cells, or many thousands of lower resolution simulations to be computed rapidly. For example, with approximately the same computing resources used to evolve a $19,278^3$ simulation one could run thousands of $2,000^3$-cell simulations, enabling entire parameter studies with resolutions comparable to current cutting edge CPU-based simulations.

% - Testing/CI stuff
In addition to requiring complex algorithms to produce accurate results, modern community-developed simulation codes like Cholla require robust testing and software-development infrastructure to maintain their reliability. This work also presents the implementation of an automated testing/continuous integration (CI) pipeline for Cholla. CI tools have expanded rapidly in functionality and popularity over the last 20 years and their usefulness in scientific software is well established \citep{beck_1999, wilson_2014,wilson_2017}. Particularly in the last 5 years with the advent of GitHub Actions and similar easily accessible and cheap (or even free) tools, CI pipelines have become much more straightforward to set up and run even for small groups and individuals. We present our implementation of testing and CI that is designed to be straightforward and scalable from a single GPU all the way up to an exascale machine. 

% - outline paper
%   - alude to performance and testing at scale
The outline of this paper is as follows. In Section \ref{sec:methods}, we describe our implementation of the VL+CT algorithm in detail along with the modifications we made to efficiently run on GPUs. In Section \ref{sec:mhd-tests}, we demonstrate the correctness and accuracy of Cholla on a suite of MHD test problems. We also describe Cholla's performance and weak scaling behavior on up to 74,088 GPUs using \textit{Frontier}. In Section \ref{sec:testing}, we discuss the new continuous integration and automated testing framework. We conclude in Section \ref{sec:summary}. In figure captions the GitHub icon, \img{../assets/github.png}, links to the version of the python script which generated that figure. These scripts, and the associated GitHub repository, have sufficient information to reproduce the figures shown in this paper.
